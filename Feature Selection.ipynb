{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction via <br>Feature Selection\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><font color=\"#000000\">\n",
    "    \n",
    "Reducing the dimensionality of the feature space not only allows learning algorithms to run much faster, but may also improve the predictive performance of a model, especially when our dataset contains a large number of features that contain noises.\n",
    "\n",
    "There are two main categories of dimensionality reduction techniques: <b>feature selection</b> and <b>feature extraction</b>.  \n",
    "\n",
    "- In feature selection, we select a subset of the original features that retain most of the information needed for a given machine learning task;\n",
    "\n",
    "- In feature extraction, we derive information from the original features and construct new features.\n",
    "\n",
    "We focus on feature selection in this notebook.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><font color=\"#000000\">\n",
    "<b><font color=\"#008000\">Concept Review: Data Transformers</font></b>\n",
    "\n",
    "Features dimensionality reduction is a <b>data transformation process</b>: It transforms or compresses the original features into fewer features while retaining most of the information.\n",
    "\n",
    "As with the two transformers (label encoder and standard scaler) we learned before, any transformer involves 'fit' and 'transform' steps:\n",
    "<ol>\n",
    "    <li><b>Fitting</b> step finds the parameters of a transformer.</li>\n",
    "    <li><b>Transforming</b> step applies the parameterized transformer to transform the data.  In dimensionality reduction, this transformation step returns a fewer number of features than pre-transformation.</li>\n",
    "</ol>\n",
    "    \n",
    "See a list of data transformers at https://scikit-learn.org/stable/data_transforms.html\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following command to widen this notebook\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><font color=\"#000000\">\n",
    "Let's first run a block of codes that we are already familiar with.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(suppress=True, linewidth=200)\n",
    "\n",
    "winedf = pd.read_csv(\"wine.csv\")\n",
    "\n",
    "# Define Predictors and Target Variable\n",
    "X = winedf.iloc[:, 1:]       # iloc selects data by index\n",
    "y = winedf.loc[:, 'Class label']\n",
    "\n",
    "# Splitting Data into Training Set and Test Set\n",
    "from sklearn.model_selection import train_test_split\n",
    "testsize = 0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=testsize, stratify=y, random_state=0)\n",
    "\n",
    "# Standardizing data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std= sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C=100, solver='liblinear', multi_class='ovr')\n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "print(\"Training accuracy: \", lr.score(X_train_std, y_train))\n",
    "print(\"Test accuracy:     \", lr.score(X_test_std, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#0000E0\"> Recursive Feature Elimination (RFE) </font>\n",
    "\n",
    "__Reference__:  \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><font color=\"#000000\">\n",
    "The idea is simple: Starting from an initial set of features, we eliminate the least important features, and repeat the procedure until the desired number of features is reached.\n",
    "\n",
    "<b>RFE</b> is readily available from the <b>feature selection</b> module in <b>scikit-learn</b> library.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C=100, solver='liblinear', multi_class='ovr')\n",
    "\n",
    "# Construct the feature selector\n",
    "from sklearn.feature_selection import RFE\n",
    "rfe = RFE(estimator=lr, n_features_to_select=2)\n",
    "\n",
    "# Select features\n",
    "rfe.fit(X_train_std, y_train)\n",
    "print('Feature ranking: Selected features are ranked 1: ', rfe.ranking_)\n",
    "print(np.vstack((rfe.ranking_, winedf.columns[1:])).T)\n",
    "X_train_rd= rfe.transform(X_train_std)\n",
    "X_test_rd = rfe.transform(X_test_std)\n",
    "\n",
    "# Verify the selected features are indeed from desired columns\n",
    "#dif = X_train_rd - X_train_std[:,[6,9]]\n",
    "#print(np.mean(dif), np.std(dif))\n",
    "\n",
    "# Train the model using only the selected features\n",
    "lr.fit(X_train_rd, y_train)\n",
    "print(\"Training accuracy: \", lr.score(X_train_rd, y_train))\n",
    "print(\"Test accuracy:     \", lr.score(X_test_rd, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><font color=\"#000000\">\n",
    "Original features must be normalized or standardized.  Otherwise, we can make any feature important or unimportant, as shown in the following codes.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the standardized features\n",
    "X_alt = np.copy(X_train_std)\n",
    "\n",
    "# Scale several features\n",
    "X_alt[:,4] = X_alt[:,4] * 100000\n",
    "X_alt[:,5] = X_alt[:,5] * 100000\n",
    "X_alt[:,6] = X_alt[:,6] / 100000\n",
    "\n",
    "# Now see how features rank\n",
    "rfe.fit(X_alt, y_train)\n",
    "print(np.vstack((rfe.ranking_, winedf.columns[1:])).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#0000E0\"> Recursive Feature Elimination with Cross Validation (RFECV) </font>\n",
    "\n",
    "__Reference__:  \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><font color=\"#000000\">\n",
    "In RFE, we specify <b>n_features_to_select</b>, but what is the best number of features to select?     \n",
    "To answer this question, you can consider a series of RFE models with different <b>n_features_to_select</b>. We can evaluate each model using cross validation and tuning the hyperparameter <b>n_features_to_select</b> to achieve best validation accuracy.  This entire process is already coded for you: <b>RFECV</b> from scikit-learn does exactly <b>R</b>ecursive <b>F</b>eature <b>E</b>limination and <b>C</b>ross-<b>V</b>alidated selection of the best number of features.   \n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C=100, solver='liblinear', multi_class='ovr')\n",
    "\n",
    "# Construct the feature selector\n",
    "from sklearn.feature_selection import RFECV\n",
    "rfecv = RFECV(estimator=lr, cv=10)\n",
    "\n",
    "# Select features\n",
    "rfecv.fit(X_train_std, y_train)\n",
    "print('Best number of features to select:', rfecv.n_features_,\n",
    "      '\\nFeature ranking: Selected features are ranked 1:\\n',\n",
    "      np.vstack((rfecv.ranking_, winedf.columns[1:])).T)\n",
    "X_train_rd= rfecv.transform(X_train_std)\n",
    "X_test_rd = rfecv.transform(X_test_std)\n",
    "\n",
    "# Train the model using only the selected features\n",
    "lr.fit(X_train_rd, y_train)\n",
    "print(\"Training accuracy: \", lr.score(X_train_rd, y_train))\n",
    "print(\"Test accuracy:     \", lr.score(X_test_rd, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv.cv_results_['mean_test_score']  # new this year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.plot(range(1, 14), rfecv.cv_results_['mean_test_score'], label='validation accuracy')\n",
    "plt.grid()\n",
    "plt.legend(loc='upper center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#0000E0\"> Least Absolute Shrinkage and Selection Operator (LASSO)</font>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"><font color=\"#000000\">\n",
    "<b><font color=\"#008000\">Concept Review </font></b>\n",
    "\n",
    "In LASSO regularization, we penalize weights by $\\frac{1}{C}\\sum_{j=1}^m |w_j|$, where the sum of absolute values of weights is known as the L1 norm of the weights.  In the figure below, any point $(w_1, w_2)$ on the edge of the diamond shape has the same penalty.  The corners of the diamond are more likely to minimize the regularized loss function.\n",
    "</font></div>\n",
    "<img src=\"LASSO.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Reference__:    \n",
    "https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection-using-selectfrommodel\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><font color=\"#000000\">\n",
    "The idea of feature selection by LASSO is simple: We select those features with non-zero weights estimated by a model with LASSO regularization.  Scikit-learn package has automated this process via <b>SelectFromModel</b>.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic model with LASSO regularization\n",
    "lr = LogisticRegression(C=0.05, solver='liblinear', multi_class='ovr', \n",
    "                        penalty='l1')\n",
    "\n",
    "# Construct the feature selector \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "selector = SelectFromModel(estimator=lr)\n",
    "\n",
    "# Select features\n",
    "selector.fit(X_train_std, y_train)\n",
    "print('Selected features: ', selector.get_support(),\n",
    "      '\\n', np.sum(selector.get_support()),\n",
    "      'features are selected:', \n",
    "      winedf.columns[1:].to_numpy()[selector.get_support()])\n",
    "#with np.printoptions(precision=2, linewidth=200):\n",
    "#    print(np.vstack((selector.estimator_.coef_, selector.get_support())))\n",
    "X_train_rd= selector.transform(X_train_std)\n",
    "X_test_rd = selector.transform(X_test_std)\n",
    "\n",
    "# Train the model using the selected features\n",
    "# Different regularization scheme can be used\n",
    "lr = LogisticRegression(C=100, solver='liblinear', multi_class='ovr')\n",
    "lr.fit(X_train_rd, y_train)\n",
    "print(\"Training accuracy: \", lr.score(X_train_rd, y_train))\n",
    "print(\"Test accuracy:     \", lr.score(X_test_rd, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#0000E0\"> Summary: Feature Selection </font>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"><font color=\"#000000\">\n",
    "    \n",
    "1. Preprocess the data and construct a machine learning model (same as what we did in previous classes)\n",
    "\n",
    "2. Construct a feature selector: RFE, RFECV, LASSO, etc.\n",
    "\n",
    "3. Select features by using 'fit' and 'transform' functions:\n",
    "\n",
    "<ul>\n",
    "<li>Fit function finds the parameters of the transformer, e.g., the feature ranking (rfe.ranking_, rfecv.ranking_), the number of features selected (rfecv.n_features_), etc.  This step should use training set only.\n",
    "</li>\n",
    "<li>Transform function collects the selected features and return them as an array.  This step applies to both training and test set.  The returned arrays have the same number of rows (observations) but fewer columns (features).\n",
    "</li>\n",
    "</ul>\n",
    "\n",
    "4. Train the model using the selected features, evaluate model performance, and tune hyperparameters.\n",
    "\n",
    "</font></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
