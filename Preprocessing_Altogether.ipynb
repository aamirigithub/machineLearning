{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data and Building Pipelines for Machine Learning Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- [Facing Reality: Missing Values and Mixed Data Types](#Facing-Reality:-Missing-Values-and-Mixed-Data-Types)\n",
    "- [Handling Missing Data (Part 1): Identification and Deletion](#Handling-Missing-Data-(Part-1):-Identification-and-Deletion)\n",
    "- [Converting Texts into Numerical Values](#Converting-Texts-into-Numerical-Values)\n",
    "- [Handling Categorical Features](#Handling-Categorical-Features)\n",
    "- [Handling Missing Data (Part 2): Imputation](#Handling-Missing-Data-(Part-2):-Imputation)\n",
    "- [Bringing Features onto the Same Scale](#Bringing-Features-onto-the-Same-Scale)\n",
    "- [Streamlining Processes Using Pipelines](#Streamlining-Processes-Using-Pipelines)\n",
    "- [Putting Everything Together](#Putting-Everything-Together)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#0000E0\">Facing Reality: Missing Values and Mixed Data Types</font>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><font color=\"#000000\">\n",
    "\n",
    "In the following simple dataset,\n",
    "\n",
    "- color and neck-style are <b>nominal</b> categorical features;\n",
    "- size is an <b>ordinal</b> categorical feature;\n",
    "- price, cotton, and sales are <b>numerical</b> or <b>continuous</b> variables;\n",
    "- many values are missing, with various missing-value codes: NaN, price = -1, and style = 99;\n",
    "- data format is not ideal: percentage of cotton should be numbers but in text format.\n",
    "\n",
    "Note: If a cell in .csv file is empty, the imported value will be <b>NaN</b>, which stands for missing value.  Although NaN literally stands for 'not a number', it is not text either.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(suppress=True, linewidth=200)\n",
    "\n",
    "# For other input function, see https://pandas.pydata.org/pandas-docs/stable/reference/io.html\n",
    "df0 = pd.read_csv('clothing_simple.csv')\n",
    "df = df0.copy()  # Making a copy will facilitate comparison. We will not change df0 and only work on df.\n",
    "\n",
    "print(df.dtypes)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><font color=\"#000000\">\n",
    "<b><font color=\"#008000\" size=3> Review: Type of target variable determines the type of machine learning task</font></b>\n",
    "\n",
    "When the target variable is continuous, we have a numerical prediction problem that can be analyzed using <b>regressions</b>.  When the target variable is categorical with two classes, we have a <b>binary classification</b> problem.  When the target variable is categorical with three or more classes, we need to distinguish between <b>nominal vs. ordinal classification</b>.\n",
    "\n",
    "Recall that if a <b>nominal</b> target variable is in text form (e.g., name of the iris classes), we can use <b>label encoder</b> to encode the target variable into numerical values.  But as we will see, nominal feature values need to be treated differently.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#0000E0\">Handling Missing Data (Part 1): Identification and Deletion</font>\n",
    "\n",
    "Reference: Chapter 4 of <i>Python Machine Learning</i>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><font color=\"#000000\">\n",
    "<b><font color=\"#0000E0\" size=3>Machine learning models require no missing data</font></b>\n",
    "\n",
    "We have learned that machine learning models analyze feature values in various ways: logistic regression, support vector machines, and linear regressions compute <b>weighted feature values</b>, k-nearest neighbors model calculates <b>distance between feature values</b>, and decision tree model finds <b>threshold feature values</b>.\n",
    "\n",
    "Missing values cannot pass through model training.  For example, weighted feature value $w_0 + w_1 x_1 + w_2 \\text{NaN}$ is meaningless unless we replace NaN by some value.\n",
    "In fact, scikit-learn's machine learning models require that no missing values are present in the input data. See https://scikit-learn.org/stable/modules/impute.html\n",
    "\n",
    "When some feature values are missing, could we discard entire rows and/or columns containing missing values?  Yes, but only sometimes, because this may come at the price of losing valuable data.\n",
    "Another strategy  is  to <b>impute</b> the missing values, i.e., to infer them from the known part of the data, which we will discuss in Part 2.\n",
    "</font></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><font color=\"#000000\">\n",
    "\n",
    "<b><font color=\"#0000E0\" size=3> Identify missing values </font></b>\n",
    "    \n",
    "Special codes are often used to indicate missing values, e.g., in a survey dataset, 95 may represent \"prefer not to answer\", 99 may represent \"I don't know\".  It is important <b>not to mistake these codes for actual values</b>.\n",
    "\n",
    "A dataframe's <b>replace</b> function can replace all the missing value codes by <b>np.nan</b> (from numpy), which is understood as missing value by computers.  \n",
    "For using the replace function, see https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html\n",
    "</font></div>\n",
    "\n",
    "<p style=\"background-color: #FFFF88; font-weight:bold\">\n",
    "    <font color=\"#0000E0\">Essential code: </font>\n",
    "    <font style=\"font-family:Consolas\">df = df.replace( { 'price': -1, 'neckstyle': 99 }, np.nan ) </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'price': -1, 'neckstyle': 99} is a dictionary. It informs the replace function to \n",
    "# look for -1 in column 'price' and 99 in column 'neckstyle' and then replace these values with the second argument\n",
    "# Here, we overwrite df with replaced values\n",
    "df = df.replace({'price': -1, 'neckstyle': 99}, np.nan)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following shortcut is okay only if -1 and 99 represent missing values for the entire dataset\n",
    "# Caution: It will cause mistakes if 99 appears in 'sales' column.\n",
    "\n",
    "print(np.sum( (df0==-1) | (df0==99) )) # checking if -1 and 99 appear somewhere else\n",
    "df0.replace([-1, 99], np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><font color=\"#000000\">\n",
    "It is useful to get a sense of how many values are missing.\n",
    "For a pandas dataframe, you can use either <b>.isnull()</b> or <b>.isna()</b> to identify missing values.  Then, you can count missing values by columns or by rows.  If an instance (row) has too many missing values, we can consider dropping that instance, while if it has only a few missing values, we can consider imputing missing values.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count missing values by columns\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count missing values by rows\n",
    "df.isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# total number of missing values\n",
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><font color=\"#000000\">\n",
    "\n",
    "<b><font color=\"#0000E0\" size=3> Delete instances (rows) or features (columns) that meet given criteria </font></b>\n",
    "\n",
    "We can require each row or column to have a given minimum number of non-missing values, and drop those that don't meet this minimum requirement.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep rows that have at least 'thresh' non-missing values \n",
    "# Change 'thresh' to test\n",
    "df.dropna(thresh=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop a row if all values are missing\n",
    "# Equivalent to dropna(thresh=1)\n",
    "df.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop a row if any value is missing\n",
    "# Equivalent to dropna(thresh=number of columns)\n",
    "df.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can ask dropna() to check the requirement only on a given subset of columns\n",
    "# For example, require no missing values in specific columns\n",
    "df.dropna(subset=['color','size','price'], how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep rows that have at least 'thresh' non-missing values in specific columns\n",
    "df.dropna(subset=['color','size','price'], thresh=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep features (columns) that have at least 'thresh' non-missing values \n",
    "df.dropna(thresh=5, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop a column if all values in that column are missing\n",
    "df.dropna(how='all', axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color: #FFFF88; font-weight:bold\">\n",
    "    <font color=\"#0000E0\">Essential code: </font>\n",
    "    <font style=\"font-family:Consolas\">df = df.dropna( how='all' ).dropna( how='all', axis='columns' ) </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting a combination of rows and columns\n",
    "# This time, we overwrite the original df\n",
    "df = df.dropna(how='all').dropna(how='all', axis='columns')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#0000E0\">Converting Texts into Numerical Values</font>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><font color=\"#000000\">\n",
    "\n",
    "Feature values don't always come in the format desirable for machine learning.  Numerical features may be stored as texts (due to symbols such as %, $, and thousands separator), while categorical features may be stored as numbers.  \n",
    "\n",
    "To check whether data types is consistent with its meaning, use dataframe's <b>dtypes</b>.  \n",
    "See data types at https://pbpython.com/pandas_dtypes.html\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the output below, 'color', 'size', and 'price' have data types consistent with their meanings.\n",
    "# ('object' type contains text or mixed text and numbers.)  \n",
    "# However, 'neckstyle' should be categorical and 'cotton' should be numerical.\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 'cotton' column.\n",
    "# We will convert the texts in 'cotton' column into numerical values.\n",
    "df['cotton']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can delete '%', but the data type is still 'object'\n",
    "df['cotton'].str.replace('%', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color: #FFFF88; font-weight:bold\">\n",
    "    <font color=\"#0000E0\">Essential code: </font>\n",
    "    <font style=\"font-family:Consolas\">df['cotton'] = pd.to_numeric( df['cotton'].str.replace('%', '') ) </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use pandas to_numeric function to convert text into numbers.\n",
    "# We overwrite the 'cotton' column\n",
    "df['cotton'] = pd.to_numeric(df['cotton'].str.replace('%',''))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data types again\n",
    "df['cotton'].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><font color=\"#000000\">\n",
    "<b><font color=\"#0000E0\" size=3> Other Resources </font></b>\n",
    "\n",
    "Converting texts into numbers can be tricky, regardless what software you use.  Here are a few more references:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html\n",
    "<b>read_excel</b> is sometimes useful because Excel specifies whether each cell contains number or text, but if a column has some numbers in text format, the resulting dataframe from read_excel needs to be checked and cleaned. \n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html pay particular attention to the section on <b>extracting substrings</b>.\n",
    "\n",
    "In working with texts, you may encounter unfamiliar syntax related to <b>regular expressions</b>.  A good tutorial is at  \n",
    "https://www.dataquest.io/blog/regular-expressions-data-scientists/\n",
    "\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#0000E0\">Handling Categorical Features</font>\n",
    "\n",
    "Reference: Chapter 4 of <i>Python Machine Learning</i>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><font color=\"#000000\">\n",
    "<b><font color=\"#0000E0\" size=3> Types of categorical features </font></b>\n",
    "\n",
    "Ordinal features (e.g., XS, S, M, L, XL in clothing sizes) can be mapped into numerical values.  Nominal features, however, cannot be simply encoded, otherwise the learning model will mistake nominal features for ordinal features. \n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our dataset\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><font color=\"#000000\">\n",
    "    \n",
    "<b><font color=\"#800000\" size=3> Incorrect mapping of ordinal features</font></b>\n",
    "\n",
    "What mistake will we make if using label encoder on an ordinal feature ('size')?   Try the following.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "size_num = le.fit_transform( df['size'].dropna() )\n",
    "print(\"Original size data:\", df['size'].dropna().to_numpy())\n",
    "print(\"Encoded size data: \", size_num)\n",
    "print(\"Label encoder orders labels alphabetically:\", le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><font color=\"#000000\">\n",
    "    \n",
    "<b><font color=\"#0000E0\" size=3> Map ordinal features: Correct approach </font></b>\n",
    "\n",
    "To make sure that our learning models interpret ordinal features correctly, we need to <b>define the mapping from the categorical texts to numerical values</b>, and then use pandas series' <b>map</b> function to apply the mapping.  It is similar to <b>replace</b> but <b>map</b> applies to series only.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color: #FFFF88; font-weight:bold\">\n",
    "    <font color=\"#0000E0\">Essential code: </font>\n",
    "    <font style=\"font-family:Consolas\">df['size'] = df['size'].map({'S':1, 'M':2, 'L':3, 'XL':4 }) </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite the dataframe with encoded 'size' column\n",
    "df['size'] = df['size'].map({'S':1, 'M':2, 'L':3, 'XL':4 })\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><font color=\"#000000\">\n",
    "    \n",
    "<b><font color=\"#800000\" size=3> Incorrect encoding of nominal features </font></b>\n",
    "\n",
    "In our dataset, 'color' and 'neckstyle' are nominal features.  If we encode colors into numbers (say 1 for blue, 2 for green, 3 for red) and feed these numbers into a training process, we will make one of the most common mistakes in dealing with categorical data, because we are imposing 'blue' < 'green' < 'red', which should not be assumed.  Same for 'neckstyle', which is already encoded and will be interpreted by computers as 1 < 2 < 3, which could imply 'V-neck' < 'crew neck' < 'scoop neck'.  These unintended orders can degrade model performance.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><font color=\"#000000\">\n",
    "    \n",
    "<b><font color=\"#0000E0\" size=3> Correct approach: \"One-hot\" encoding on nominal features </font></b>\n",
    "\n",
    "One-hot encoding is a simple idea that is illustrated in the block below.  For each category, we create an indicator feature. Only one indicator feature is set to 1 in each row (hence the name 'one-hot'), and 0 elsewhere.\n",
    "\n",
    "Note that one-hot encoding is exactly the same as using dummy variables in statistics.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()\n",
    "df1 = df1.replace({'color': np.nan}, 'green')\n",
    "df1 = df1.replace({'neckstyle': np.nan}, 3)\n",
    "df1[['color', 'neckstyle']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color: #FFFF88; font-weight:bold\">\n",
    "    <font color=\"#0000E0\">Essential code: </font>\n",
    "    <font style=\"font-family:Consolas\"> OneHotEncoder( handle_unknown='ignore' ) </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# handle_unknown='ignore':  When an unknown category is encountered during transform, \n",
    "# the resulting one-hot code for this category will be all zeros.\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "ohe.fit_transform( df1[['color']].to_numpy() ).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding scheme\n",
    "ohe.transform([['blue'], ['green'], ['red']]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><font color=\"#000000\">\n",
    "<b>Caveat</b>: One-hot encoding creates the 'multicollinearity' problem, which can lead to arbitrarily large weights in ordinary linear regressions. (For example, if a model involves weighted feature values: $- w_0 + w_1 \\text{blue} + w_2 \\text{green} + w_3 \\text{red} + w_4 ...$, then increasing $w_0, w_1, w_2, w_3$ by the same amount will not change the weighted feature value.)  Some people suggest dropping one indicator feature.\n",
    "    \n",
    "But if our model is trained with regularization (ridge, lasso, elastic net), keeping all indicator features won't be an issue.  In fact, we should keep all indicator features if our model prediction depends on which column we drop.\n",
    "See more discussions at https://inmachineswetrust.com/posts/drop-first-columns/\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#0000E0\">Handling Missing Data (Part 2): Imputation</font>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><font color=\"#000000\">\n",
    "\n",
    "All of the above data preprocessing steps (deleting rows and columns, converting texts into numerical values, and encoding categorical features) may be performed before splitting data into training and test sets.\n",
    "\n",
    "Imputation is to replace missing data with substituted values, e.g., mean or median or mode (most frequent value) of the non-missing values. The mean, median, or mode should be calculated based on the training set only, or based on 9 folds during the 10-fold cross validation, otherwise imputed values may contain information from the test or validation set.\n",
    "\n",
    "To illustrate imputation, we don't split data below, but treat our dataset as the training set.  We will use scikit-learn's <b>SimpleImputer</b>. Its usage is consistent with all other data transformers.   \n",
    "See https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color: #FFFF88; font-weight:bold\">\n",
    "    <font color=\"#0000E0\">Essential code: </font>\n",
    "    <font style=\"font-family:Consolas\"> SimpleImputer( strategy='mean' or 'median' or 'most_frequent')</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_num = df[['cotton','price']]\n",
    "print(X_train_num,  '  Original data\\n')\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "X1 = imp.fit_transform(X_train_num)\n",
    "print(X1, ' Mean imputation\\n')\n",
    "\n",
    "imp = SimpleImputer(strategy='median')\n",
    "X1 = imp.fit_transform(X_train_num)\n",
    "print(X1, ' Median imputation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cat = df[['size', 'neckstyle', 'color']]\n",
    "print(X_train_cat,  ' Original data\\n')\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(strategy='most_frequent')\n",
    "X2 = imp.fit_transform(X_train_cat)\n",
    "print(X2, ' Mode imputation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><font color=\"#000000\">    \n",
    "\n",
    "<b>Caveats</b>: If a nominal feature has missing values, an alternative to imputation is to let 'missing value' be another category. After one-hot encoding, the column corresponding to the 'missing value' category can be dropped. \n",
    "\n",
    "\n",
    "Note that SimpleImputer does not allow different imputation strategies for different columns.  We could do this manually, but scikit-learn already has a solution: <b>Column Transformer</b>, which we will introduce in [Streamlining Processes Using Pipelines](#Streamlining-Processes-Using-Pipelines).\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><font color=\"#000000\"> \n",
    "   \n",
    "<b><font color=\"#0000E0\" size=3> More resources </font></b>\n",
    "\n",
    "Imputing missing values can be a learning process by itself.  For example, we can try to estimate a missing value as a function of other features; we can also apply the idea of k-nearest neighbors: identify k neighbors based on the non-missing values, and then impute the missing value based on the feature values of the neighbors.  More resources can be found at\n",
    "\n",
    "https://scikit-learn.org/stable/modules/impute.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.impute\n",
    "\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#0000E0\">Bringing Features onto the Same Scale</font>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><font color=\"#000000\">\n",
    "\n",
    "Majority of machine learning models perform much better if features are on the same scale.\n",
    "Two common approaches: \n",
    "\n",
    "- Standardization: Rescale a feature so that it has mean 0 and std dev 1: $x^{(i)}_{std} = (x^{(i)} - \\mu_x)\\ /\\ {\\sigma_x}$ (equivalent to the z-score)\n",
    "\n",
    "- Normalization: Rescale a feature to the unit range [0,1]:\n",
    "$x^{(i)}_{norm} = (x^{(i)} - x^{(i)}_{min})\\ /\\ ( x^{(i)}_{max} - x^{(i)}_{min}) $\n",
    "\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit_transform(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mms = MinMaxScaler()\n",
    "mms.fit_transform(X1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#0000E0\">Streamlining Processes Using Pipelines</font>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><font color=\"#000000\">\n",
    "\n",
    "The following two blocks of codes put together all of the above data preprocessing steps.\n",
    "\n",
    "- The first block collects the first four lines of \"essential codes\" highlighted in yellow;\n",
    "\n",
    "- The second block utilizes pipelines to streamline data transformations.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(suppress=True, linewidth=200)\n",
    "\n",
    "df = pd.read_csv('clothing_simple.csv')\n",
    "\n",
    "df = df.replace({'price': -1, 'neckstyle': 99}, np.nan)         # Identify missing data\n",
    "df = df.dropna(how='all').dropna(how='all', axis='columns')     # Delete some missing data\n",
    "df['cotton'] = pd.to_numeric(df['cotton'].str.replace('%',''))  # Convert texts into numbers\n",
    "df['size'] = df['size'].map({'S':1, 'M':2, 'L':3, 'XL':4 })     # Map ordinal categories to numbers\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><font color=\"#000000\">\n",
    "\n",
    "<b><font color=\"#0000E0\" size=3> Data preprocessing in the machine learning framework</font></b>\n",
    "\n",
    "Steps that are independent of how we split data can be performed before splitting data:\n",
    " \n",
    "- Deleting rows and columns that have no valid data\n",
    "- Converting text into numbers\n",
    "- Mapping ordinal features into numbers\n",
    "- Other nonlinear transformation such logarithmic transformation \n",
    "\n",
    "Steps that depend on how we split data should be performed after splitting data:\n",
    "\n",
    "- Imputing missing data by mean, median, or mode\n",
    "- One-hot encoding (in principle can be done early, but better do this after imputation)\n",
    "- Standardization and normalization \n",
    "\n",
    "</font></div>\n",
    "\n",
    "<img src=\"Overall_process.png\" width=600>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><font color=\"#000000\">\n",
    "\n",
    "<b><font color=\"#0000E0\" size=3> Pipeline and Column Transformer</font></b>\n",
    "\n",
    "We will build a <b>main pipeline</b> with three <b>parallel branches</b> processing three different types of features.\n",
    "A <b>column transformer</b> will send the right columns to the right branches.  Explanations are provided along with the codes below.   \n",
    "Refer to https://scikit-learn.org/stable/data_transforms.html for more details.\n",
    "\n",
    "\n",
    "</font></div>\n",
    "\n",
    "<img src=\"Pipeline_branches.png\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline and column transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "# Data transformers\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# Learning model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "nom_col = ['color','neckstyle'] # nominal features\n",
    "ord_col = ['size']              # ordinal features\n",
    "num_col = ['cotton', 'price']   # numerical features\n",
    "\n",
    "# We assume df is the training data, otherwise split data here\n",
    "X_train = df[nom_col + ord_col + num_col]\n",
    "y_train = df['sales']\n",
    "\n",
    "# Make three branches (you can make more branches to meet different preprocessing needs)\n",
    "# Branch for nominal features\n",
    "nom_pipe = make_pipeline(SimpleImputer(strategy='most_frequent'),\n",
    "                         OneHotEncoder(handle_unknown='ignore')\n",
    "                        )\n",
    "# Branch for ordinal features\n",
    "ord_pipe = make_pipeline(SimpleImputer(strategy='median'),\n",
    "                         StandardScaler()\n",
    "                        )\n",
    "# Branch for numerical features\n",
    "num_pipe = make_pipeline(SimpleImputer(strategy='mean'),\n",
    "                         MinMaxScaler()\n",
    "                        )\n",
    "# Make the main pipe, in which a column transformer sends 'nom_col' into 'nom_pipe', etc.\n",
    "pipe = make_pipeline(ColumnTransformer( [ ('nom', nom_pipe, nom_col),\n",
    "                                          ('ord', ord_pipe, ord_col),\n",
    "                                          ('num', num_pipe, num_col) ] ),\n",
    "                     # PCA( n_components = 3 ),\n",
    "                     # Can use any suitable learning model here\n",
    "                     Lasso( alpha=0.001, max_iter=5000 )\n",
    "                     # LinearRegression()\n",
    "                    )\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "with(np.printoptions(precision=2, floatmode='fixed')):\n",
    "    print('Predicted y:', pipe.predict(X_train))\n",
    "    print('   Actual y:', y_train.values)\n",
    "print('Training score:', pipe.score(X_train,y_train))\n",
    "\n",
    "print('\\n blue    green    red     sty1    sty2    sty3     size   cotton   price')\n",
    "\n",
    "with(np.printoptions(precision=3)):\n",
    "    print(pipe.named_steps.lasso.coef_, '(coef), %.4f (intercept)'%pipe.named_steps.lasso.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><font color=\"#000000\">\n",
    "To see the input data into the ML model, first <b>comment out</b> the machine learning model in the previous block, and then run the following codes.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' blue    green    red     sty1    sty2    sty3     size   cotton   price')\n",
    "with(np.printoptions(precision=4)):\n",
    "    print( pipe.transform(X_train) )\n",
    "X_train  # Compare with input data into the main pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#0000E0\">Putting Everything Together</font>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><font color=\"#000000\">\n",
    "\n",
    "We look at a machine learning project using a <b>Titanic dataset</b> from https://www.kaggle.com/c/titanic\n",
    "We will use the above pipeline framework to build our machine learning model, which will be trained and cross validated.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(suppress=True, linewidth=200)\n",
    "\n",
    "df = pd.read_csv('titanic_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><font color=\"#000000\"> \n",
    "   \n",
    "<b><font color=\"#0000E0\" size=3> Exploratory Data Analysis </font></b>\n",
    "\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()  # identify missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.hist(bins=12, figsize=(14,6), layout=(2,-1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()  # descriptive statistics of the numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=['O'])    # descriptive statistics of the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Embarked'].value_counts()  # counts for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Gender'] = df['Sex'].map({'male':0, 'female':1 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.pairplot(df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "columns = df.describe().columns\n",
    "cm = df[columns].corr()   # Correlation matrix\n",
    "sns.set(font_scale=1)\n",
    "plt.figure(figsize=(6,6))\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=columns, xticklabels=columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><font color=\"#000000\"> \n",
    "   \n",
    "<b><font color=\"#0000E0\" size=3> Building Pipeline Framework: Titanic Project </font></b>\n",
    "\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(suppress=True, linewidth=200)\n",
    "\n",
    "# Pipeline and column transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "# Data transformers\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# Data splitter and model evaluator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "# Learning models (use one of them or any other model)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "df = pd.read_csv('titanic_train.csv')       # import data\n",
    "\n",
    "nom_col = ['Sex', 'Embarked']               # nominal features\n",
    "ord_col = ['Pclass']                        # ordinal features\n",
    "num_col = ['Age', 'SibSp', 'Parch', 'Fare'] # numerical features\n",
    "\n",
    "# We assume df is the training data, otherwise split data here\n",
    "X_train = df[nom_col + ord_col + num_col]\n",
    "y_train = df['Survived']\n",
    "\n",
    "# Branch for nominal features\n",
    "nom_pipe = make_pipeline(SimpleImputer(strategy='most_frequent'),\n",
    "                         OneHotEncoder(handle_unknown='ignore') )\n",
    "# Branch for ordinal features\n",
    "ord_pipe = make_pipeline(SimpleImputer(strategy='median'),\n",
    "                         StandardScaler())\n",
    "# Branch for numerical features\n",
    "num_pipe = make_pipeline(SimpleImputer(strategy='mean'),\n",
    "                         StandardScaler())\n",
    "# Main pipe \n",
    "pipe = make_pipeline(ColumnTransformer( [ ('nom', nom_pipe, nom_col),\n",
    "                                          ('ord', ord_pipe, ord_col),\n",
    "                                          ('num', num_pipe, num_col) ] ),\n",
    "                     #PCA(n_components=3),\n",
    "                     SVC(kernel='rbf', C=1000, gamma=1)\n",
    "                     #LogisticRegression(solver='lbfgs', C=0.01)\n",
    "                     #DecisionTreeClassifier(criterion='gini', max_depth=3)\n",
    "                     #RandomForestClassifier(criterion='gini', n_estimators=20, random_state=1)\n",
    "                     #KNeighborsClassifier(n_neighbors=5, p=2)\n",
    "                    )\n",
    "\n",
    "pipe.fit(X_train,y_train)\n",
    "\n",
    "print('Training score:', pipe.score(X_train,y_train))\n",
    "\n",
    "#pipe.named_steps.logisticregression.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_name  = 'svc__gamma'\n",
    "param_range = np.logspace(-5, 1, 13)\n",
    "#param_name  = 'logisticregression__C'\n",
    "#param_range = np.logspace(-4, 2, 13)\n",
    "#param_name  = 'decisiontreeclassifier__max_depth'\n",
    "#param_range = np.arange(1,15)\n",
    "#param_name  = 'randomforestclassifier__max_depth'\n",
    "#param_range = np.arange(1,15)\n",
    "#param_name  = 'kneighborsclassifier__n_neighbors'\n",
    "#param_range = np.arange(1,26,2)\n",
    "\n",
    "\n",
    "train_scores, val_scores = validation_curve(estimator=pipe, X=X_train, y=y_train, \n",
    "                                            cv=10,\n",
    "                                            param_name=param_name, \n",
    "                                            param_range=param_range)\n",
    "\n",
    "trn_mean = np.mean(train_scores, axis=1)\n",
    "trn_std  = np.std (train_scores, axis=1)\n",
    "val_mean = np.mean(val_scores, axis=1)\n",
    "val_std  = np.std (val_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(param_range, trn_mean, 'bo-',  markersize=5, label='training accuracy')\n",
    "plt.fill_between(param_range, trn_mean+trn_std, trn_mean-trn_std, alpha=0.25, color='blue')\n",
    "\n",
    "plt.plot(param_range, val_mean, 'gs--', markersize=5, label='validation accuracy')\n",
    "plt.fill_between(param_range, val_mean+val_std, val_mean-val_std, alpha=0.15, color='green')\n",
    "\n",
    "plt.grid()\n",
    "plt.xscale('log')  # Use this only when param_range = np.logspace(...). Comment this out otherwise.\n",
    "plt.legend(loc='lower center', fontsize=14)\n",
    "plt.xlabel(param_name, fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "#plt.savefig('val_curve')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
